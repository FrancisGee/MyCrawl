#Baidutieba Crawl
My second crawl based on Python,used to get all the posts on tieba.baidu.com((for example)[http://tieba.baidu.com/p/3138733512?see_lz=0&pn=1] ),then write them to the disk file

##Notable Features

* Written in Python 2.X
* Based on the libarary `re`,`urllib`,`urllib2`
* choose any resource in **tieba.baidu.com** when you input the specific *resource-code*
* choose whether only see the posts written by OP
* save all the posts into disk file

##Features in future
* get the image of "posts"
* support Python 3.X
* accelerate the crawl speed

##How to use
* Use `python tieba.py` to launch the program
* Enter the *resource-code* for which website you want to crawl,ex *3138733512*
* choose if you want only see the posts written by OP or get the floor info
* just wait for secs and then find **title.txt** in your workspace

## Contributor 
* [FrancisGee](https://github.com/FrancisGee)

**Welcome to *open issues* to make good suggestions and join in** 

## License

The Apache License 2.0

## Last Updata
2016/08/29
